{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8587c25",
   "metadata": {},
   "source": [
    "# PEFT - LoRA\n",
    "\n",
    "https://huggingface.co/docs/peft/en/developer_guides/lora\n",
    "\n",
    "\n",
    "**1. 다양한 초기화 전략 (Initialization)**\n",
    "\n",
    "LoRA 가중치를 어떻게 시작하느냐에 따라 학습 속도와 성능이 달라진다.\n",
    "\n",
    "* **기본값:** 가중치 A는 Kaiming-uniform, B는 0으로 초기화하여 처음에는 정등 변환(Identity transform) 상태로 시작한다.\n",
    "* **PiSSA:** 주특이값(Principal singular values)을 사용하여 초기화하며, 일반 LoRA보다 수렴 속도가 빠르고 성능이 우수하다.\n",
    "* **OLoRA:** QR 분해를 활용하여 베이스 모델의 가중치를 변환하며, 학습 안정성과 수렴 속도를 높인다.\n",
    "* **EVA:** 입력 활성화 값에 대해 SVD를 수행하여 데이터 기반으로 초기화하며, 레이어별로 랭크(Rank)를 유연하게 할당한다.\n",
    "\n",
    "**2. 양자화 모델 최적화 (LoftQ & DoRA)**\n",
    "\n",
    "* **LoftQ:** 양자화된 모델을 미세 조정할 때 발생하는 오차를 최소화하도록 LoRA 가중치를 초기화하는 기술이다.\n",
    "* **DoRA (Weight-Decomposed Low-Rank Adaptation):** 가중치 업데이트를 크기(Magnitude)와 방향(Direction)으로 분리하여 처리하며, 특히 낮은 랭크에서도 높은 성능을 보여준다.\n",
    "\n",
    "**3. 효율적인 추론 및 학습 기법**\n",
    "\n",
    "* **aLoRA (Activated LoRA):** 특정 토큰(invocation tokens)이 나타날 때만 어댑터를 활성화하는 방식이다. 기본 모델과 KV 캐시를 공유할 수 있어 추론 속도를 획기적으로 높일 수 있다.\n",
    "* **Rank-stabilized LoRA (rsLoRA):** 랭크()의 제곱근에 비례하여 스케일링을 조절함으로써, 높은 랭크를 사용할 때 학습을 안정화시킨다.\n",
    "* **레이어 복제 (Layer Replication):** 기존 모델의 레이어를 논리적으로 복제하여 모델 크기를 확장하되, 실제 메모리 사용량은 최소화하면서 어댑터를 추가하는 방식이다.\n",
    "\n",
    "**4. 고급 최적화 및 제어**\n",
    "\n",
    "* **특수 옵티마이저:** 가중치 A를 고정하고 B만 튜닝하여 메모리를 아끼는 **LoRA-FA**, A와 B에 서로 다른 학습률을 적용해 속도를 2배 높이는 **LoRA+** 등을 지원한다.\n",
    "* **세부 제어:** 특정 레이어마다 서로 다른 랭크()나 알파() 값을 지정할 수 있는 기능을 제공한다.\n",
    "* **토큰 학습:** 특정 레이어의 가중치뿐만 아니라, 특정 토큰 임베딩만 선택적으로 학습시키는 기능을 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a60c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq transformers datasets accelerate trl peft hf_transfer pydantic langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi  # GPU 모델명 / 드라이브 버전 / 메모리 사용량 등 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local 실행인 경우 아래 코드로 키를 설정\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runpod 실행인 경우 아래 코드로 키를 설정\n",
    "# import os\n",
    "\n",
    "# OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "# HF_TOKEN = os.environ[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5809d",
   "metadata": {},
   "source": [
    "## 데이터셋 로드\n",
    "https://huggingface.co/datasets/capybaraOh/naver-economy-news2stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20549ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  # HuggingFace 데이터셋 로더\n",
    "\n",
    "# Hub에서 split이 train인 데이터 로드\n",
    "dataset = load_dataset('capybaraOh/naver-economy-news2stock', split='train')\n",
    "print(len(dataset))  # 샘플 개수\n",
    "print(dataset)       # 객체 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5876450",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[100]  # 101번째 샘플을 확인 (딕셔너리)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfc309",
   "metadata": {},
   "source": [
    "## 데이터셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6898b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2  # 평가셋 비율\n",
    "\n",
    "train_data = []   # 학습 데이터 리스트\n",
    "test_data = []    # 테스트 데이터 리스트\n",
    "\n",
    "data_indices = list(range(len(dataset)))       # 전체 인덱스\n",
    "test_size = int(len(dataset) * test_ratio)     # 테스트 셋 크기\n",
    "\n",
    "test_data_indices = data_indices[:test_size]   # 앞부분은 평가셋으로 사용\n",
    "train_data_indices = data_indices[test_size:]  # 나머지는 학습셋으로 사용\n",
    "\n",
    "# 학습/평가 데이터셋 형식을 지정하는 함수\n",
    "def format_data(data):\n",
    "    # OpenAI / Chat 학습용 messages 포맷 반환\n",
    "    return {\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': data['system']\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': data['user']\n",
    "            },\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': data['assistant']\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# 학습 / 평가 인덱스를 변환해 리스트 생성\n",
    "train_data = [format_data(dataset[i]) for i in train_data_indices]\n",
    "test_data = [format_data(dataset[i]) for i in test_data_indices]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ded9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[100])  # 학습 데이터의 101번째 샘플 확인\n",
    "print(type(train_data))  # train_data는 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323520f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset  # Huggfing Face Dataset 컨테이너\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)  # train_data(list) -> Dataset 변환\n",
    "test_dataset = Dataset.from_list(test_data)    # test_data(list) -> Dataset 변환\n",
    "\n",
    "print(train_dataset[100])\n",
    "print(type(train_dataset))  # train_dataset은 Dataset형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd902bcd",
   "metadata": {},
   "source": [
    "## NCSOFT/Llama-VARCO-8B-Instruct란?\n",
    "https://huggingface.co/NCSOFT/Llama-VARCO-8B-Instruct\n",
    "\n",
    "\n",
    "* **기반 모델:** Meta의 Llama-3.1-8B 모델을 기반으로 한다.\n",
    "* **개발 목적:** 한국어 능력을 극대화하는 동시에 영어 구사 능력도 유지하도록 설계되었다.\n",
    "* **학습 방법:** 한국어와 영어 데이터셋을 활용한 지속 사전 학습(Continual Pre-training)을 거쳤으며, 이후 지도 미세 조정(SFT)과 직접 선호도 최적화(DPO)를 통해 인간의 선호도에 맞게 정렬되었다.\n",
    "\n",
    "\n",
    "**SFT에서 한국어능력향상과 동시에 영어능력유지란:**\n",
    "\n",
    "일반적으로 한국어 데이터를 대량으로 추가 학습시키면 기존에 모델이 가지고 있던 영어 지식이 손상되는 '파괴적 망각(Catastrophic Forgetting)' 현상이 발생한다. 엔씨소프트는 이를 방지하기 위해 **지속 사전 학습(Continual Pre-training)**을 적용했다.\n",
    "\n",
    "**_1. 데이터 믹스(Data Mixing) 전략:_**\n",
    "\n",
    "단순히 한국어 데이터만 밀어 넣는 것이 아니라, 모델이 이미 학습했던 영어 데이터와 고품질의 한국어 데이터를 특정 비율로 섞어 학습한다. 이를 통해 기존의 영어 추론 능력을 '복습'하면서 새로운 언어 체계를 '습득'하게 된다.\n",
    "\n",
    "**_2. 토크나이저 효율화와 임베딩 확장:_**\n",
    "\n",
    "기존 Llama-3.1의 토크나이저 성능을 유지하면서 한국어 표현력을 높이기 위해 어휘 사전(Vocabulary)을 최적화한다. 영어 토큰 정보는 건드리지 않고 한국어 토큰의 밀도를 높여 두 언어 간의 연결 고리를 강화하는 방식이다.\n",
    "\n",
    "**_3. 지식 전이(Knowledge Transfer):_**\n",
    "\n",
    "영어 데이터로 학습된 모델의 강력한 논리적 사고 능력을 한국어로 전이시키는 과정을 거친다.\n",
    "\n",
    "* **추론 능력 유지:** 수학이나 코딩 같은 논리적 작업은 영어 데이터에서 배운 구조를 그대로 활용한다.\n",
    "* **언어 정렬:** SFT(지도 미세 조정) 단계에서 동일한 질문을 한국어와 영어로 번급하며 학습시켜, 언어에 상관없이 일관된 답변을 내놓도록 유도한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # 토크나이저 / 생성형 모델 로더\n",
    "import torch\n",
    "\n",
    "pretrained_model_name = 'NCSOFT/Llama-VARCO-8B-Instruct'  # 사용할 사전학습 모델 ID\n",
    "\n",
    "# 사전학습 CausalLM 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name,   # 모델 이름\n",
    "    dtype = torch.bfloat16,  # 가중치 로딩 dtype(bf16)\n",
    "    device_map = 'auto'      # 환경에 맞게 CPU/GPU 자동 배치\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)  # 해당 모델의 토크나이저 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12549137",
   "metadata": {},
   "source": [
    "## llama-3 chat template 변환\n",
    "\n",
    "Llama3 모델은 특정 chat template 형식으로 학습되어, 그 형식을 사용해야 최적 성능을 낼 수 있다.\n",
    "Chat template을 사용하지 않으면 모델이 대화 구조를 제대로 인식하지 못할 수 있다.\n",
    "opean_ai 형식의 데이터를 llama-3 형식으로 변환한다.\n",
    "\n",
    "\n",
    "**LLaMA-3 채팅 포맷**\n",
    "LLaMA-3 채팅 포맷은 LLaMA-3 계열 챗봇 모델이 대화 내용을 이해하고 답변할 수 있도록 만들어진 입력 데이터 구조입니다.\n",
    "여러 역할(시스템, 유저, 어시스턴트)의 메시지를 특별한 토큰과 구조로 묶어서 하나의 프롬프트로 합치는 방식입니다.\n",
    "구조 예시\n",
    "아래와 같이 대화 흐름을 명확히 구분하는 토큰들이 사용됩니다:\n",
    "\n",
    "```\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "[시스템 역할 지침]<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "[유저 질문]<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "[모델의 답변]<|eot_id|>\n",
    "```\n",
    "* <|begin_of_text|> : 전체 프롬프트의 시작을 알리는 토큰\n",
    "* <|start_header_id|>role<|end_header_id|> : 각 메시지의 역할 구분(시스템, 유저, 어시스턴트 등)\n",
    "* 각 메시지 끝에 <|eot_id|> : 하나의 메시지 블록이 끝났음을 알림\n",
    "* 마지막 assistant 블럭은 응답 생성 위치를 가리킨다. apply_chat_template(add_generation_prompt=False)로 설정했더라도 내부 템플릿에는 응답을 받을 자리 표시자로 <|assistant|> 토큰이 남아 있어, \"여기서부터 어시스턴트가 답변을 생성해야 한다\"는 신호를 제공하는 것임.\n",
    "\n",
    "**왜 이 포맷이 필요할까?**\n",
    "\n",
    "* 모델이 **“어디까지가 시스템 안내, 어디서부터가 유저 질문, 어디서부터가 답변인지”** 정확하게 파악할 수 있다.\n",
    "* 여러 턴(turn)의 대화가 이어질 때도 메시지 경계를 명확히 구분해 혼동 없이 맥락을 유지할 수 있다.\n",
    "* LLaMA-3 계열 모델은 이런 포맷으로 학습되어 있기 때문에 **실전 파인튜닝/추론 시에도 반드시 이 구조로 입력해야** 기대하는 챗봇 성능을 발휘할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_chat_template 함수\n",
    "# - openai 방식의 메시지를 llama3 방식으로 변환\n",
    "text = tokenizer.apply_chat_template(train_dataset[100]['messages'], tokinize=False)  # messages -> 채팅 프롬프트 문자열 변환\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790f1b2",
   "metadata": {},
   "source": [
    "### data_collator 함수\n",
    "\n",
    "* 미니배치(batch) 데이터를 모델이 바로 학습할 수 있는 형태(토큰·마스크·정답)로 변환합니다.\n",
    "* 특히 아래와 같은 LLaMA-3 채팅 포맷을 쓸 때,\n",
    "  “어디까지가 질문/어디서부터가 답변(assistant)인지”를 정확히 구분해서\n",
    "  모델이 정답(답변 부분)만 학습하도록 레이블을 지정합니다.\n",
    "\n",
    "#### 함수 설명\n",
    "\n",
    "**1. 프롬프트 생성 (Prompt Construction)**\n",
    "\n",
    "입력받은 `batch` 데이터는 리스트 내에 여러 메시지(`system`, `user`, `assistant`)를 포함하는 사전(dict) 구조이다.\n",
    "\n",
    "* Llama 3의 특수 토큰(` <|begin_of_text|>`, `<|start_header_id|>`, `<|eot_id|>`)을 사용하여 모든 대화 내용을 하나의 긴 문자열로 병합한다.\n",
    "* 각 역할(role)의 시작과 끝을 명확히 구분하여 모델이 대화 맥락을 이해할 수 있도록 구성한다.\n",
    "\n",
    "**2. 토크나이즈 및 패딩 (Tokenization)**\n",
    "\n",
    "병합된 문자열 리스트를 `tokenizer`를 통해 숫자 ID(`input_ids`)로 변환한다.\n",
    "\n",
    "* `padding=True`: 배치 내의 문장들 중 가장 긴 문장을 기준으로 길이를 맞춘다.\n",
    "* `truncation=True`: `max_length`를 초과하는 데이터는 절단한다.\n",
    "* `return_tensors=\"pt\"`: PyTorch 텐서 형식으로 결과를 반환한다.\n",
    "\n",
    "**3. 레이블 생성 및 Loss Masking**\n",
    "\n",
    "이 함수의 핵심 부분이다. 모델이 '사용자의 질문'이 아닌 **'모델의 답변(assistant)'** 부분에 대해서만 학습하도록 설정한다.\n",
    "\n",
    "* **-100 값의 의미**: PyTorch의 `CrossEntropyLoss`는 레이블 값이 `-100`인 경우 손실(Loss) 계산에서 제외한다. 이를 통해 모델은 질문 부분을 예측하려고 노력하지 않고, 답변 부분의 정확도에만 집중하게 된다.\n",
    "* **구간 탐색**: `assistant_tokens`를 기점으로 답변이 시작되는 위치를 찾고, `<|eot_id|>` 토큰이 나오는 지점까지의 인덱스를 추출한다.\n",
    "* **값 복사**: 해당 구간의 `labels`에만 실제 `input_ids` 값을 복사하여 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat 모델 학습용 데이터 콜레이터: 프롬프트 생성 -> 토크나이저/패딩 -> assistant 구간만 라벨링\n",
    "\n",
    "# 배치(messages)를 모델 학습 텐서로 변환하는 함수\n",
    "def data_collator(batch, tokenizer=tokenizer, max_length=8192):\n",
    "    # 1. 프롬프트 생성\n",
    "    prompts = []  # 배치프롬프트 문자열을 담을 리스트\n",
    "    for example in batch:\n",
    "        prompt = '<|begin_of_text|>'  # 프롬프트 시작\n",
    "        for msg in example['messages']:  # system/user/assistant 순회\n",
    "            role = msg['role']\n",
    "            content = msg['content'].strip()\n",
    "            # role+content를 템플릿으로 누적\n",
    "            prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "        prompts.append(prompt)  # 완성된 프롬프트를 배치 리스트에 추가\n",
    "    \n",
    "    # 2. 토큰처리/패딩/텐서변환\n",
    "    tokenized = tokenizer(        # 프롬프트를 토큰화해서 텐서로 변환\n",
    "        prompts,                  # 배치 프롬프트 목록\n",
    "        truncation = True,        # 최대길이 초과시 자름\n",
    "        max_length = max_length,  # 최대 토큰 길이\n",
    "        padding = True,           # 배치 내 최장 길이 기준 패딩\n",
    "        return_tensors = \"pt\"     # Pytorch 텐서 반환\n",
    "    )\n",
    "    input_ids = tokenized['input_ids']  # 토큰 id 텐서\n",
    "    attention_mask = tokenized['attention_mask']  # 패딩 마스크 텐서\n",
    "\n",
    "    # 3. 라벨 생성\n",
    "    labels = torch.full_like(input_ids, fill_value=-100)  # 기본은 -100(손실 계산에서 제외)\n",
    "\n",
    "    assistant_header = '<|start_header_id|>assistant<|end_header_id|>'\n",
    "    assistant_token_id = tokenizer.encode(assistant_header, add_special_tokens=False)  # 헤더의 토큰 패턴\n",
    "    eot_token = '<|eot_id|>'\n",
    "    eot_token_id = tokenizer.encode(eot_token, add_special_tokens=False)  # 종료 토큰의 토큰 패턴\n",
    "\n",
    "    for i, ids in enumerate(input_ids):  # 배치 내 각 샘플별로 라벨 구간 설정\n",
    "        ids_list = ids.tolist()          # 슬라이싱 비교를 위한 리스트 변환\n",
    "        # assistant 답변 시작위치 찾기\n",
    "        # - <|start_header_id|>assistant<|end_header_id|>\\n 다음 인덱스부터 답변 시작\n",
    "        start = None  # 답변 시작 인덱스 초기화\n",
    "        for idx in range(len(ids_list) - len(assistant_token_id) + 1):  # 헤더 길이만큼 슬라이딩 탐색\n",
    "            if ids_list[idx: idx + len(assistant_token_id)] == assistant_token_id:  # 헤더 패턴 매칭\n",
    "                start = idx + len(assistant_token_id)  # 헤더 다음 토큰부터 라벨 시작\n",
    "                break  # 첫 번째 assistant 구간만 탐색\n",
    "        # 답변 끝 위치 찾기\n",
    "        # - <|eot_id|> 까지\n",
    "        if start is not None:  # assistant 헤더를 찾은 경우\n",
    "            end = None  # 답변 종료 인덱스 초기화\n",
    "            # start 이후부터 <|eot_id|> 패턴이 나오는 곳까지를 탐색\n",
    "            for idx in range(start, len(ids_list) - len(eot_token_id) + 1):\n",
    "                if ids_list[idx: idx + len(eot_token_id)] == eot_token_id:  # 종료 패턴 매칭\n",
    "                    end = idx + len(eot_token_id)  # eot까지 포함한 구간 설정\n",
    "                    break  # 첫 번째 eot 탐색 완료시 종료\n",
    "        \n",
    "        labels[i, start:end] = input_ids[i, start:end]  # assistant 답변 구간을 정답 라벨로 복사\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,            # 모델 입력\n",
    "        'attention_mask': attention_mask,  # 패딩 마스크\n",
    "        'labels': labels                   # 손실 계산용 라벨(assistant 구간)\n",
    "    }\n",
    "\n",
    "data_collator([train_dataset[0], train_dataset[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3f335",
   "metadata": {},
   "source": [
    "### Causal Language Model 파인튜닝: input_ids와 labels 구조 이해\n",
    "\n",
    "**데이터 구조**\n",
    "```\n",
    "input_ids:  [system_tokens..., user_tokens..., assistant_tokens...]  # 전체 시퀀스\n",
    "labels:     [-100, -100, ..., -100, assistant_tokens...]          # assistant만 학습 대상\n",
    "```\n",
    "\n",
    "| 항목 | 내용 |\n",
    "| --- | --- |\n",
    "| **Input IDs** | 프롬프트 + 정답 (전체 시퀀스) |\n",
    "| **Labels** | `-100` (프롬프트 구간) + 정답 토큰 (답변 구간) |\n",
    "| **결과** | 모델은 입력을 다 보지만, 오직 답변을 맞히는 과정에서만 학습이 일어남 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> **질문에 해당하는 input_ids에 이미 답이 포함되어 있다!**\n",
    ">\n",
    "> **\"답이 이미 있는데 어떻게 학습하는가?\"**\n",
    ">\n",
    "> 모델은 정답을 \"보면서\" 각 위치에서 올바른 다음 토큰을 예측하는 법을 배운다. 마치 학생이 모범답안을 보며 \"이 상황에서는 이렇게 답해야 한다\"를 학습하는 것과 같다. 이것이 현대 LLM 파인튜닝의 핵심 메커니즘이다!\n",
    "\n",
    "\n",
    "**_1. 인과적 언어 모델링 (Causal Language Modeling):_**\n",
    "\n",
    "LLM(Llama, GPT 등)은 **이전 토큰들을 보고 다음 토큰을 예측**하는 방식으로 학습한다. 따라서 학습 데이터에는 프롬프트와 정답이 모두 포함된 전체 문장이 들어가야 한다.\n",
    "\n",
    "* **학습 원리:** 모델은 번째 토큰까지를 입력으로 받아 번째 토큰을 예측한다.\n",
    "* **구조:** `input_ids`가 `[A, B, C, D]`라면, 모델은 내부적으로 `A`를 보고 `B`를, `A, B`를 보고 `C`를 예측하는 과정을 동시에 수행한다.\n",
    "\n",
    "**_2. Teacher Forcing 기법:_**\n",
    "```\n",
    "Position:   [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "input_ids:  [A, B, C, D, E, F, G, H, I]\n",
    "labels:     [-100, -100, -100, -100, E, F, G, H, I]\n",
    "```\n",
    "\n",
    "학습 과정:\n",
    "- Position 4: A,B,C,D를 보고 → E 예측\n",
    "- Position 5: A,B,C,D,E를 보고 → F 예측  \n",
    "- Position 6: A,B,C,D,E,F를 보고 → G 예측\n",
    "\n",
    "**_3. Labels와 Loss 계산의 역할:_**\n",
    "\n",
    "`input_ids`에 정답이 포함되어 있더라도, 모델이 모든 구간에 대해 학습(손실 계산)을 수행하는 것은 아니다. 이때 중요한 역할을 하는 것이 바로 코드에 작성된 **`labels`**이다.\n",
    "\n",
    "* **-100의 의미:** PyTorch의 `CrossEntropyLoss`는 기본적으로 레이블 값이 `-100`인 위치를 무시(ignore)한다.\n",
    "* **학습 차단:** 코드에서 프롬프트(User 질문 등) 구간의 레이블을 `-100`으로 설정했기 때문에, 모델이 프롬프트 내용을 예측하며 발생하는 오차는 학습에 반영되지 않는다.\n",
    "* **학습 집중:** 오직 `assistant`의 답변 구간에 해당하는 `labels`만 실제 `input_ids` 값을 가지므로, 모델은 **\"프롬프트가 주어졌을 때 정답을 생성하는 방법\"**에 대해서만 가중치를 업데이트한다.\n",
    "\n",
    "\n",
    "**학습 vs 추론의 차이**\n",
    "\n",
    "**_학습 시:_**\n",
    "```\n",
    "input_ids: <system>당신은 금융분석가</system><user>뉴스내용</user><assistant>분석결과</assistant>\n",
    "labels:    [-100, -100, ..., -100, 분석결과_토큰들]\n",
    "```\n",
    "\n",
    "**_추론 시:_**\n",
    "```\n",
    "input:  <system>당신은 금융분석가</system><user>뉴스내용</user><assistant>\n",
    "output: 분석결과 (모델이 한 토큰씩 생성)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b357e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset[128]               # 129번째 샘플\n",
    "batch = data_collator([example])           # 배치 1개로 콜레이터 적용\n",
    "print(f'{batch['input_ids'].shape}')       # input_ids 텐서 shape 확인\n",
    "print(f'{batch['attention_mask'].shape}')  # attention_mask 텐서 shape 확인\n",
    "print(f'{batch['labels'].shape}')          # labels 텐서 shape 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['input_ids'][0].tolist())       # 샘플의 input_ids를 리스트로 확인\n",
    "print(batch['attention_mask'][0].tolist())  # 샘플의 attention_mask를 리스트로 확인\n",
    "print(batch['labels'][0].tolist())          # 샘플의 labels를 리스트로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeeb8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = [token_id for token_id in batch['labels'][0].tolist() if token_id != -100]  # 정답 토큰(-100 제외)만 추출\n",
    "text = tokenizer.decode(label_ids)  # 정답 토큰을 문장열로 디코딩\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][0].tolist())  # 토큰 ID를 토큰 문자열로 변환\n",
    "\n",
    "text_tokens = []  # 토큰 ID를 1개씩 디코딩한 문자열을 담을 리스트\n",
    "for i, token_id in enumerate(batch['input_ids'][0].tolist()):  # 토큰 ID를 순회\n",
    "    decoded_str = tokenizer.decode([token_id])  # 토큰 1개를 문자열로 디코딩\n",
    "    text_tokens.append(decoded_str)             # 디코딩 결과를 누적 저장\n",
    "\n",
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'token': text_tokens,  # 토큰 (1개씩 디코딩된 문자열)\n",
    "    'input_ids': batch['input_ids'][0].tolist(),  # 입력 토큰 ID\n",
    "    'attention_mask': batch['attention_mask'][0].tolist(),  # 패딩 여부(1/0)\n",
    "    'labels': batch['labels'][0].tolist()  # 정답 라벨(-100 마스킹 포함)\n",
    "}).transpose()  # 컬럼을 행으로 전치\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # 출력시 컬럼 생략 없이 표시\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180db77a",
   "metadata": {},
   "source": [
    "## PEFT Finetuning - LoRA\n",
    "\n",
    "* LoRA는 **\"Low-Rank Adapter(저랭크 어댑터)\"**\n",
    "* 거대한 대형언어모델(LLM)의 **전체 파라미터를 일일이 미세조정(파인튜닝)하지 않고**,\n",
    "  **딱 필요한 핵심 부분만 저렴하게 빠르게 학습**하는 최신 파인튜닝.\n",
    "* **\"LLM의 성능은 그대로, 비용/시간/메모리/유지보수는 최소로\"** 파인튜닝을 할 수 있게 해주는 AI 실무에서 가장 중요한 기법 중 하나이다.\n",
    "\n",
    "**왜 LoRA가 등장했을까?**\n",
    "\n",
    "* GPT, Llama, DeepSeek 같은 대형언어모델은 **파라미터(매개변수) 수가 수십억\\~수조 개**나 된다.\n",
    "* 이런 모델을 파인튜닝하려면 **막대한 GPU 메모리와 시간, 저장 공간**이 필요.\n",
    "* 하지만, 실제로 특정 태스크에 맞게 모델을 조정할 때 **전체를 다 바꿀 필요가 없다.**\n",
    "* 대부분의 정보는 기존 모델에 이미 들어있고,\n",
    "  **특정 입력(질문)과 특정 출력(답변)의 관계만 살짝 조정**해주면 충분하다.\n",
    "\n",
    "**LoRA의 원리**\n",
    "\n",
    "* 기존 대형 모델의 핵심 연산(주로 \"곱셈\" 부분)에\n",
    "  **작고 얇은 \"보조 네트워크(어댑터 레이어)\"**를 덧붙인다.\n",
    "* 전체 모델은 거의 건드리지 않고,\n",
    "  **이 어댑터 레이어의 파라미터만 새로 추가해서 학습**\n",
    "* 학습이 끝나면,\n",
    "\n",
    "  * 원본 모델은 그대로\n",
    "  * 어댑터(작은 추가 파라미터)만 별도로 저장하면 끝!\n",
    "* 추론할 땐 **원본 모델 + LoRA 어댑터**를 합쳐서 쓸 수 있다.\n",
    "\n",
    "**LoRA의 장점**\n",
    "\n",
    "* **파인튜닝 비용(시간, 메모리, 저장 용량)이 압도적으로 절약**된다.\n",
    "* 7B, 13B, 70B 등 대형 모델도\n",
    "  **일반 GPU(24GB/48GB)로도 쉽게 파인튜닝**이 가능하다.\n",
    "* **동일한 원본 모델에 다양한 LoRA 어댑터만 바꿔 끼우며\n",
    "  다양한 분야별 파인튜닝 결과를 쉽게 쓸 수 있다.**\n",
    "\n",
    "**LoRA와 기존 방식의 비교**\n",
    "\n",
    "* **기존 파인튜닝:**\n",
    "  전체 파라미터(수십\\~수백 GB)를 새로 저장/관리/학습 → 비효율적\n",
    "* **LoRA:**\n",
    "  원본은 그대로 두고,\n",
    "  변화가 필요한 부분(수 MB\\~수십 MB)만 별도로 학습/저장\n",
    "\n",
    "\n",
    "**실전에서의 활용 예시**\n",
    "\n",
    "* 번역 LoRA, 요약 LoRA, 감정분석 LoRA 등\n",
    "  **하나의 원본 모델에 여러 용도별 어댑터를 저장/관리**할 수 있다.\n",
    "* **A100 80GB, 3090, T4 등 다양한 GPU 환경에서도\n",
    "  고성능 LLM 튜닝이 매우 쉽게 가능하다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model  # LoRA 설정 / PEFT 적용 함수\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,               # 저랭크 행렬 rank\n",
    "    lora_alpha = 32,     # LoRA 스케일 계수 (alpha/r)\n",
    "    lora_dropout = 0.1,  # 드롭아웃 비율\n",
    "    bias = 'none',       # bais 학습 안함\n",
    "    target_modules = ['q_proj', 'v_proj'],  # LoRA를 주입할 모듈\n",
    "    task_type = 'CAUSAL_LM'  # 작업 유형(생성)\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)  # 기존 모델에 LoRA 어댑터 적용\n",
    "model.print_trainable_parameters()          # 학습 가능한 파라미터 수/비율 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTConfig\n",
    "from trl import SFTConfig  # TRL SFT 학습 설정 클래스\n",
    "\n",
    "hub_model_id = 'capybaraOh/Llama-VARCO-8b-news2stock-analyzer'  # 학습 완료 후 업로드할 Hub 모델 ID\n",
    "\n",
    "sft_config = SFTConfig(  # SFT 학습 하이퍼파라미터/저장/로그 설정\n",
    "    output_dir=\"Llama-VARCO-8b-news2stock-analyzer\", # 학습 완료된 모델과 체크포인트가 저장될 경로이다.\n",
    "    num_train_epochs=3,                              # 전체 데이터셋을 반복 학습할 횟수(Epoch)이다.\n",
    "    per_device_train_batch_size=2,                   # 각 GPU(장치)당 한 번에 처리할 데이터 샘플의 개수이다.\n",
    "    gradient_accumulation_steps=2,                   # 그래디언트를 2번 누적한 후 가중치를 업데이트한다. (실제 배치 크기 = 2 * 2 = 4 효과를 낸다.)\n",
    "    gradient_checkpointing=True,                     # VRAM 절약을 위해 중간 활성화 값을 저장하지 않고 역전파 시 재계산하는 설정이다.\n",
    "    optim=\"adamw_torch_fused\",                       # 최적화 알고리즘 설정이다. fused 버전은 CUDA에서 더 빠르다.\n",
    "    logging_steps=10,                                # 10 스텝마다 학습 로그(Loss 등)를 출력한다.\n",
    "    save_strategy=\"steps\",                           # 체크포인트 저장 기준을 'steps'(스텝 수)로 설정한다. (옵션: 'epoch')\n",
    "    save_steps=100,                                  # 100 스텝마다 모델 체크포인트를 저장한다.\n",
    "    bf16=True,                                       # BF16(Brain Float 16) 정밀도를 사용하여 메모리를 아끼고 연산 속도를 높인다. (Ampere GPU 이상 권장)\n",
    "    learning_rate=1e-4,                              # 학습률(Learning Rate)이다. 가중치 업데이트의 크기를 결정한다.\n",
    "    max_grad_norm=0.3,                               # 그래디언트 클리핑 임계값이다. 그래디언트 폭주를 막아 학습 안정성을 높인다.\n",
    "    warmup_ratio=0.03,                               # 전체 학습 단계의 3% 동안 학습률을 서서히 올리는 웜업(Warmup)을 수행한다.\n",
    "    lr_scheduler_type=\"constant\",                    # 학습률 스케줄러 타입이다. 여기서는 학습률을 변동 없이 상수로 유지한다.\n",
    "    push_to_hub=True,                                # 학습이 끝나면 Hugging Face Hub에 모델을 자동으로 업로드한다.\n",
    "    hub_model_id=hub_model_id,                       # Hub에 업로드될 때 사용될 저장소(Repository) ID이다.\n",
    "    hub_token=True,                                  # Hub 업로드를 위해 인증 토큰을 사용한다.\n",
    "    remove_unused_columns=False,                     # 데이터셋에서 모델의 forward 메서드 시그니처에 없는 컬럼을 자동으로 삭제하지 않도록 한다.\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},   # 데이터셋 처리 과정(packing 등)을 건너뛰도록 하는 설정이다.\n",
    "    report_to=[],                                    # 학습 기록을 전송할 툴(WandB, Tensorboard 등)을 지정한다. 빈 리스트는 기록하지 않음을 의미한다.\n",
    "    label_names=[\"labels\"],                          # 손실(Loss) 계산 시 정답(Target)으로 사용할 데이터셋의 컬럼 이름이다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer  # SFT 학습용 Trainer\n",
    "\n",
    "# SFT 학습 객체 생성\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                  # 학습할(LoRA 적용된) 모델\n",
    "    args = sft_config,              # SFT 학습 설정\n",
    "    train_dataset = train_dataset,  # 학습 데이터셋\n",
    "    data_collator = data_collator   # 배치 텐서 생성 함수\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e328965",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4321e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋 messages에서 프롬프트/정답(assistant) 텍스트 분리\n",
    "prompt_list = []   # 프롬프트(assistant메시지 이전) 저장 리스트\n",
    "label_list = []    # 정답 (assistant 답변) 저장 리스트\n",
    "\n",
    "for messages in test_dataset['messages']:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)  # 채팅 템플릿 문자열로 변환\n",
    "    # assistant 시작 전까지는 프롬프트로 사용 + assistant 헤더만 프롬프트 끝에 붙여준다.\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + \\\n",
    "        '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    # assistant 답변(종료 토큰 전)만 추출\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    \n",
    "    prompt_list.append(input)\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f674a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a51503",
   "metadata": {},
   "source": [
    "### 추론모델 - 런타임결합\n",
    "1. lora모델을 로드\n",
    "2. base모델 + lora adapter 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f293e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM  # Hub에 저장된 PEFT 모델 로더\n",
    "from transformers import AutoTokenizer, pipeline  # 토크나이저 / 파이프라인 생성\n",
    "import torch\n",
    "\n",
    "peft_model_id = hub_model_id\n",
    "\n",
    "finetuned_model = AutoPeftModelForCausalLM.from_pretrained(  # 파인튜닝 된 PEFT 모델 로드\n",
    "    peft_model_id,\n",
    "    dtype = torch.bfloat16,  # bf16 로드\n",
    "    device_map = 'auto'      # CPU/GPU 자동 배치\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)  # 같은 repo에서 토크나이저 로드\n",
    "pipe = pipeline('text-generation', model=finetuned_model, tokenizer=tokenizer)  # 텍스트 생성 파이프라인\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06796262",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer('<|eot_id|>', add_special_tokens=False)['input_ids'][0]  # <|eot_id|>의 토큰ID 추출\n",
    "eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트를 입력받으면 assistant 생성 결과만 반환하는 함수\n",
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)  # 응답을 생성\n",
    "    assistant_start = len(prompt)  # 생성 결과에서 프롬프트 길이만큼은 입력 구간\n",
    "    return outputs[0]['generated_text'][assistant_start:].strip()  # 프롬프트 이후(생성된 부분)만 잘라서 반환\n",
    "\n",
    "for prompt, label in zip(prompt_list[10:13], label_list[10:13]):  # 10~12 샘플 비교\n",
    "    print(f'[prompt]\\n{prompt}')\n",
    "    print(f'[label]\\n{label}')\n",
    "    print(f'[response]\\n{test_inference(pipe, prompt)}')  # 모델 생성 응답 출력\n",
    "    print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11afa20",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70086285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 본문을 넣으면 모델의 분석 응답 텍스트를 반환하는 함수\n",
    "def inference(news):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': '''\n",
    "당신은 금융/경제 뉴스의 핵심내용을 요약해 설명하고,\n",
    "특정 상장 종목에 미치는 긍정/부정 영향여부, 이유, 근거를 분석하는 금융/경제 분석 전문가입니다.\n",
    "\n",
    "다음 출력지시사항을 지켜주세요.\n",
    "1. 뉴스와 종목간의 연관성을 발견할 수 없다면:\n",
    "    - stock_related를 False로 작성하세요.\n",
    "    - summary에 뉴스의 요약을 작성하세요.\n",
    "2. 뉴스와 종목간의 연관성을 발견했다면:\n",
    "    - stock_related를 True로 작성하세요.\n",
    "    - summary에 뉴스의 요약을 작성하세요.\n",
    "    - 긍정영향이 예상되는 종목이 있다면, positive_stocks, positive_keywords, positive_reasons를 작성하세요.\n",
    "    - 부정영향이 예상되는 종목이 있다면, negative_stocks, negative_keywords, negative_reasons를 작성하세요.\n",
    "    - 값이 없는 경우 빈 문자열(''), 빈 리스트([])로 작성하세요.\n",
    "'''},  # 시스템 지시문\n",
    "        {'role': 'user', 'content': news}  # 사용자 입력(뉴스)        \n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)  # messages를 채팅 프롬프트 문자열로 변환\n",
    "    # 파이프라인으로 텍스트 생성 수행\n",
    "    outputs = pipe(\n",
    "        prompt,  # 변환된 프롬프트\n",
    "        max_new_tokens = 1024,    # 생성 최대 토큰 수\n",
    "        eos_token_id = eos_token, # 종료 토큰 ID\n",
    "        do_sample = False         # False: Greedy, True: 샘플링 적용\n",
    "    )\n",
    "    assistant_start = len(prompt)  # 프롬프트 길이 계산\n",
    "    return outputs[0]['generated_text'][assistant_start:].strip()  # 프롬프트 이후(생성된 부분)만 잘라서 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e4f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 1건을 inference 함수로 분석\n",
    "news = '''\n",
    "강훈식 청와대 비서실장이 이끄는 캐나다 방산 특사단 출장 기간 현지에서 한·캐나다 자동차 포럼이 열린다. 한국과 캐나다의 자동차 산업 협력에 대한 논의가 이뤄질 전망이다. 이자리에는 정의선 현대자동차그룹 회장 등 주요 경영진도 참석할 것으로 알려졌다.\n",
    "\n",
    "26일 자동차 업계 및 정부 관계자에 따르면 정 회장과 장재훈 부회장은 이번주 캐나다에서 열리는 ‘한국·캐나다 자동차 산업 협력 포럼’에 참석한다. 이 행사에는 김정관 산업통상자원부 장관과 멜라니 졸리 캐나다 산업부 장관 등 주요 인사가 모여 자동차 산업 협력 방안에 대해 논의할 예정이다. 정 회장이 현지 일정상 참석이 어려울 경우 장재훈 현대차 부회장만 참석할 가능성도 남아있다.\n",
    "\n",
    "정 회장은 현지에서 구체적인 캐나다 투자 방안을 공개할 예정이다. 현대차는 캐나다 자원 등 장점을 활용해 수소 분야를 포함한 다양한 협력 방안을 검토 중인 것으로 알려졌다. 캐나다 정부가 요구해 온 ‘전기차 전용 공장 건설’은 투자 명단에서 제외하기로 가닥이 잡혔다. 북미 시장 공략을 위해 지난해 초 미국 조지아주에 완공한 메타플랜트아메리카(HMGMA)와의 중복 투자를 피하기 위해서다.\n",
    "\n",
    "정 회장은 60조원 규모의 캐나다 초계 잠수함 사업(CPSP) 수주를 지원하기 위해 캐나다 출장길에 올랐다. 정 회장이 전면에 나선 배경에는 ‘절충교역’이 있다. 절충교역은 대규모 방산 계약을 발주하는 국가가 수주국에 현지 투자나 기술이전, 공급망 구축 등을 요구하는 방식이다. 캐나다 정부는 3000t급 디젤 잠수함 12척을 도입하는 대가로 현대차의 현지 투자를 강력히 희망해 왔다. 캐나다는 한국과 경쟁 상대인 독일 측에도 폭스바겐의 현지 생산 확대를 입찰 조건으로 제시한 것으로 전해졌다. 현대차는 캐나다에 생산 시설이 없는 반면 독일 폭스바겐은 배터리 셀 공장을 건설 중이다.\n",
    "\n",
    "이번 수주전의 성패는 향후 30년간의 유지·보수·정비(MRO) 시장 주도권과 직결된다. 이 사업은 디젤 잠수함 최대 12척을 건조하는 프로젝트다. 건조비용(약 20조원)에 도입 후 30년간 유지·보수·운영(MRO) 비용까지 포함하면 최대 60조원까지 규모가 커질 전망이다.\n",
    "'''\n",
    "inference(news)  # news 문자열을 입력으로 넣어 모델 분석 결과(assistant 응답)를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 1건을 inference 함수로 분석\n",
    "news = '''\n",
    "달러당 원화값이 26일 전 거래일 대비 20원 가까이 급등하며 1440원대를 이어가고 있다.\n",
    "\n",
    "이날 서울외환시장에서 원화값은 전 거래일보다 19.7원 오른 1446.1원에 출발해 오전 10시25분 현재 1446.2원에 거래되고 있다.\n",
    "\n",
    "미국과 일본 외환당국의 시장 개입 가능성이 거론되면서 엔화가 급등한 점이 원화 강세로 이어졌다는 분석이 나온다. 일본은행은 최근 외환시장 개입에 앞서 주요 은행을 상대로 거래 상황을 점검하는 ‘레이트 체크’를 실시한 것으로 전해졌다. 미국 뉴욕 연방준비은행도 미 재무부 지시에 따라 레이트 체크에 나섰다는 보도가 나왔다.\n",
    "\n",
    "다카이치 사나에 일본 총리는 “투기적이고 비정상적인 움직임에 필요한 모든 조처를 할 것”이라고 밝힌 바 있다. 이에 따라 지난주 달러당 160엔에 육박했던 달러당 엔화값은 155엔대 초반까지 상승했다. 현재 엔화값은 전 거래일 대비 0.50% 오른 155.04엔이다.\n",
    "\n",
    "한편 이날 열리는 국민연금 기금운용위원회에서 환헤지 전략 등이 논의될 예정이어서 원화값에 어떤 영향을 미칠지도 주목된다.\n",
    "'''\n",
    "inference(news)  # news 문자열을 입력으로 넣어 모델 분석 결과(assistant 응답)를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b23f39",
   "metadata": {},
   "source": [
    "### Base모델과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ea42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 파인튜닝 전(Base) vs 파인튜닝 후(LoRA) 모델 답변 비교\n",
    "from transformers import AutoModelForCausalLM, pipeline  # 모델 로드/파이프라인 생성\n",
    "\n",
    "base_model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\"  # 베이스(파인튜닝 전) 모델 ID\n",
    "base_model = AutoModelForCausalLM.from_pretrained(  # 베이스 모델 로드\n",
    "    base_model_id,  # 모델 ID\n",
    "    dtype=torch.bfloat16,  # BF16 로드\n",
    "    device_map=\"auto\",  # CPU/GPU 자동 배치\n",
    ")\n",
    "base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)  # 베이스 모델 파이프라인 생성\n",
    "\n",
    "for idx, (prompt, label) in enumerate(zip(prompt_list[10:13], label_list[10:13])):  # 샘플 3개만 비교\n",
    "    print(f\"[샘플 {idx + 1}]\")  # 샘플 번호 출력\n",
    "    base_resp = test_inference(base_pipe, prompt)  # 베이스 모델 응답 생성\n",
    "    lora_resp = test_inference(pipe, prompt)  # LoRA 모델 응답 생성\n",
    "    print(f\"  [Base - 파인튜닝 전]\\n{base_resp}\")  # 베이스 모델 응답 출력\n",
    "    print(f\"  [LoRA - 파인튜닝 후]\\n{lora_resp}\")  # LoRA 모델 응답 출력\n",
    "    print(f\"  [Label]\\n{label}\")  # 정답(레이블) 출력\n",
    "    print(\"-\" * 50)  # 구분선 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b3145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace에 자동으로 업로드 되지 않을 경우 수동으로 업로드\n",
    "# import os\n",
    "# from huggingface_hub import HfApi  # Hub API 사용\n",
    "\n",
    "# repo_id = 'capybaraOh/naver-economy-news2stock'      # 업로드할 모델 REPO ID\n",
    "# local_dir = \"./Llama-VARCO-8b-news2stock-analyzer\"   # 로컬 모델 폴더 경로\n",
    "\n",
    "# api = HfApi(token=os.environ[\"HF_TOKEN\"])  # Hub 인증 토큰으로 api 객체 생성\n",
    "# api.create_repo(repo_id=repo_id, repo_type='model', exist_ok=True)  # repository가 없으면 생성 (있으면 그대로 사용)\n",
    "\n",
    "# # 로컬 폴더를 Hub에 업로드\n",
    "# api.upload_folder(\n",
    "#     folder_path = local_dir,  # 업로드할 로컬 폴더\n",
    "#     repo_id = repo_id,        # 대상 repository\n",
    "#     repo_type = 'model',      # 모델 repository에 업로드\n",
    "#     ignore_patterns = ['chekckpoint-*', '**/checkpoint-*'],  # 체크포인트 폴더들은 제외\n",
    "#     commit_message = \"Upload final LoRA adapter (without checkpoints)\"  # 커밋 메시지\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
