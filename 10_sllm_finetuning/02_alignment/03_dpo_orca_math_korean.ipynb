{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61b5f07",
   "metadata": {},
   "source": [
    "# DPO orca-math-korean\n",
    "\n",
    "**Dataset:**\n",
    "- https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k\n",
    "- https://huggingface.co/datasets/kuotient/orca-math-korean-dpo-pairs\n",
    "\n",
    "**Model:**\n",
    "- https://huggingface.co/soka0000/vclm-korean-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627d756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers hf_transfer accelerate peft trl wandb scikit-learn -Uq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98772ca2",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "656a73d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9678e68e3bee47f58c333db9f0849afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085fe08dd04e4c4692697306b37465c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/162M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce253a1464fd4ee09577a1469487ca76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/192848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "{'system': '당신은 인공지능 어시스턴트입니다.', 'question': '한 배럴에는 12리터(L)와 400밀리리터(ml)의 석유, B 배럴에는 7600밀리리터(ml)의 석유가 들어 있습니다. A 배럴과 B 배럴의 석유 양을 같게 하려면 A 배럴에서 B 배럴로 몇 리터(L)를 옮겨야 합니까?', 'chosen': '먼저 모든 측정값을 동일한 단위로 변환하여 계산을 쉽게 해봅시다. 모든 것을 밀리리터(ml)로 변환하겠습니다.\\n\\n배럴에는 12리터와 400밀리리터가 들어 있습니다. 1리터는 1000밀리리터와 같으므로 12리터를 밀리리터로 변환할 수 있습니다:\\n12리터 = 12 * 1000밀리리터 = 12000밀리리터\\n\\n이제 이미 밀리리터 단위로 표시된 400밀리리터를 더합니다:\\n12000밀리리터 + 400밀리리터 = 12400밀리리터\\n\\n따라서 배럴에는 총 12400밀리리터의 석유 가 들어 있습니다.\\n\\nB 배럴에는 7600밀리리터의 석유가 들어 있습니다.\\n\\n두 배럴의 석유 양을 동일하게 하려면 두 양의 평균을 구해야 합니다:\\n두 배럴의 총량 = 12400밀리리터(A배럴) + 7600밀리리터(B배럴)\\n두 배럴의 총량 = 20000밀리리터\\n\\n이제 이 총량을 2로 나누어 각 배럴에 해당하는 양을 구합니다:\\n각 배럴의 동일한 양 = 20000밀리리터 / 2\\n각 배럴의 동일한 양 = 10000밀리리터\\n\\n현재 A 배럴에는 12400밀리리터가 있으므로, 두 배럴의 양이 각각 10000밀리리터가 되도록 일부를 B 배럴로 이동해야 합니다.\\n\\nA에서 B로 이동할 양 = 12400밀리리터(A 배럴) - 10000밀리리터(동일한 양)\\nA에서 B로 이동할 양 = 2400밀리리터\\n\\n따라서 두 배럴의 석유 양을 동일하게 하기 위해 A 배럴에서 B 배럴로 2400밀리리터(또는 2.4리터)를 이동해야 합니다.', 'rejected': ' 먼저 B 배럴의 석유 양을 리터로 변환하여 A 배럴의 석유 양과 비교해야 합니다.\\n\\nB 배럴의 석유 양은 7600밀리리터(ml)입니다. 1리터에는 1000밀리리터가 있으므로 7600밀리리터를 리터로 변환할 수 있습니다:\\n\\n7600ml = 7600 / 1000 L\\n7600ml = 7.6L\\n\\n이제 A 배럴에는 12리터와 400밀리리터의 석유가 들어 있다는 것을 알 수 있습니다. 400밀리리터를 리터로 변환해야 합니다:\\n\\n400ml = 400 / 1000 L\\n400ml = 0.4L\\n\\n따라서 A 배럴의 석유 총량은 다음과 같습니다:\\n\\n12L + 0.4L = 12.4L\\n\\nA 배럴과 B 배럴의 석유 양을 같게 하려면 A 배럴에서 B 배럴로 석유를 옮겨야 합니다. A 배럴의 석유 양은 12.4L이고 B 배럴의 석유 양은 7.6L이므로, 두 양이 같으려면 A 배럴에서 B 배럴로 12.4L - 7.6L을 옮겨야 합니다:\\n\\n12.4L - 7.6L = 4.8L\\n\\n따라서 A 배럴에서 B 배럴로 4.8리터의 석유를 옮겨야 두 배럴의 석유 양이 같아집니다.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  # HuggingFace 데이터셋 로드 함수\n",
    "\n",
    "# DPO 용 데이터 (chosen/rejected) 학습 데이터 로드\n",
    "dataset = load_dataset('kuotient/orca-math-korean-dpo-pairs', split='train')\n",
    "SAMPLE_SIZE = 10000\n",
    "dataset = dataset.select(range(SAMPLE_SIZE))  # 앞에서부터 10000개만 선택\n",
    "print(len(dataset))\n",
    "print(dataset[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83921c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': '당신은 인공지능 어시스턴트입니다.', 'question': '제임스는 일주일 동안 500개의 깡통을 모아 깡통 집을 짓기로 결심합니다. 첫날에는 50개의 깡통을 모았습니다. 둘째 날에는 첫날에 모은 깡통의 특정 배수를 모았습니다. 셋째 날에는 둘째 날에 모은 깡통보다 50개를 더 모았습니다. 그는 일주일의 나머지 날에도 매일 50개씩 같은 수의 깡통을 모았습니다. 둘째 날에 모은 깡통 수와 첫째 날에 모은 깡통 수의 비율은 얼마입니까?', 'chosen': '두 번째 날에 제임스가 수집한 깡통의 수를 첫 번째 날에 수집한 깡통의 수에 \\\\( x \\\\) 배로 표시해 봅시다. 따라서 둘째 날에 그는 \\\\( 50x \\\\) 깡통을 수집합니다.\\n\\n셋째 날에는 \\\\( 50x - 50 \\\\) 깡통을 수집합니다.\\n\\n남은 4일(4일부터 7일까지) 동안 그는 매일 50개의 깡통을 수집하여 총 4 \\\\times 50 = 200 개의 깡통을 수집합니다.\\n\\n한 주 동안 수집한 깡통의 총 개수는 매일 수집한 깡통의 합으로, 500개의 깡통과 같아야 합니다. 따라서 우리는 다음과 같습니다:\\n\\n\\\\[ 50 + 50x + (50x - 50) + 200 = 500 \\\\]\\n\\n유사한 항을 결합하면 다음과 같습니다:\\n\\n\\\\[ 50x + 50x + 200 = 500 \\\\]\\n\\n\\\\[ 100x + 200 = 500 \\\\]\\n\\n양쪽에서 200을 빼면 다음과 같습니다:\\n\\n\\\\[ 100x = 300 \\\\]\\n\\n양변을 100으로 나누면 다음과 같습니다:\\n\\n\\\\[ x = 3 \\\\]\\n\\n따라서 둘째 날에 수집한 깡통의 수와 첫째 날에 수집한 깡통의 수의 비율은 \\\\( x:1, 즉 3:1입니다.', 'rejected': ' 둘째 날에 제임스가 모은 깡통의 수를 \\\\( x \\\\)`로 표시해 봅시다.\\n\\n첫째 날에는 50개의 깡통을 모았습니다.\\n둘째 날에는 \\\\( x \\\\)개의 깡통을 모았습니다.\\n셋째 날에는 \\\\( x + 50 \\\\)개의 깡통을 모았습니다.\\n\\n일주일 동안 모은 깡통의 총 개수는 500개입니다. 따라서 방정식을 쓸 수 있습니다:\\n\\n\\\\[ 50 + x + (x + 50) + 50 + 50 + 50 = 500 \\\\]\\n\\n유사한 항을 결합하면 다음과 같습니다:\\n\\n\\\\[ 50 + x + x + 50 + 50 + 50 = 500 \\\\]\\n\\\\[ 250 + 2x = 500 \\\\]\\n\\n양쪽에서 250을 빼면 다음과 같습니다:\\n\\n\\\\[ 2x = 250 \\\\]\\n\\n양변을 2로 나누면 다음과 같습니다:\\n\\n\\\\[ x = 125 \\\\]\\n\\n따라서 제임스는 둘째 날에 125개의 깡통을 모았습니다.\\n\\n이제 둘째 날에 모은 깡통 수와 첫째 날에 모은 깡통 수의 비율을 구하기 위해 둘째 날의 깡통 수를 첫째 날의 깡통 수로 나눕니다:\\n\\n\\\\[ \\\\frac{x}{50} = \\\\frac{125}{50} \\\\]\\n\\n분수를 단순화하면 다음과 같습니다:\\n\\n\\\\[ \\\\frac{x}{50} = 2.5 \\\\]\\n\\n따라서 둘째 날에 모은 깡통 수와 첫째 날에 모은 깡통 수의 비율은 2.5:1입니다.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5f000f",
   "metadata": {},
   "source": [
    "## 모델 준비\n",
    "\n",
    "https://huggingface.co/soka0000/vclm-korean-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee1a048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665128ad935140f38b8a33838173ed4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f034445bdd4aefbea1b01cdafe3f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_soka.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/soka0000/vclm-korean-7b:\n",
      "- modeling_soka.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ba2da7e2014f12b98e7edc5474fb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cbe5e3286141809f90096e59820917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71278bc0dae440a9c01e387d4faf18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9672fa56778f495c85a352ca3b9ba450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fefa85052fa431da0a926a1d3aeaf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4f53ad76104fa4b74bf621e15c0196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814668ef473b4453b24d330505e9f63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854a4c0a1d4c454aa61f55e30a92b4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78af0bf22bb4440895f79620bc9e00ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf68949798644f4a51195210d341b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Policy(학습 대상) 모델 / 토크나이저 로드\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # 생성형 모델 / 토크나이저 자동 로드\n",
    "\n",
    "model_name = 'soka0000/vclm-korean-7b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # 토크나이저 로드 (커스텀 코드 허용)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype = torch.bfloat16,    # 메모리 절약 및 연산 효율성\n",
    "    device_map = \"auto\",       # CPU/GPU 자동 배치\n",
    "    trust_remote_code = True   # 모델 저장소에 있는 커스텀 모델 코드 허용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b82e6",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96faecfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92489b23221d4625ae6573560f63e5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: ['system', 'question', 'chosen', 'rejected']\n",
      "after: ['system', 'question', 'chosen', 'rejected', 'prompt']\n"
     ]
    }
   ],
   "source": [
    "# 원본 샘플(system/question/chosen/rejected) -> prompt/chosen/rejected 형태의 모델용 학습 데이터 형태로 바꾸는 함수\n",
    "def preprocess_dpo_data(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": example['system']},  # 시스템 프롬프트\n",
    "        {\"role\": \"user\", \"content\": example['question']}   # 사용자 질문\n",
    "    ]\n",
    "\n",
    "    # 모델이 기대하는 포맷으로 변환 (|im_start|system...|im_start|user...). 답변 생성 시작 지점까지 모두 포함\n",
    "    prompt_style = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # DPOTrainer가 사용하기 좋은 형태\n",
    "    return {\n",
    "        'prompt' : prompt_style,           # 템플릿이 적용된 프롬프트\n",
    "        'chosen' : example['chosen'],      # 선호 응답\n",
    "        'rejected' : example['rejected'],  # 비선호 응답\n",
    "    }\n",
    "\n",
    "dataset_preprocessed = dataset.map(preprocess_dpo_data)  # 전체 데이터셋에 전처리 함수 적용\n",
    "\n",
    "print(f'before: {dataset.column_names}')\n",
    "print(f'after: {dataset_preprocessed.column_names}')  # prompt 컬럼은 추가 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1804a851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n당신은 인공지능 어시스턴트입니다.<|im_end|>\\n<|im_start|>user\\n정국이 5위입니다. 정국보다 결승선을 먼저 통과한 사람의 수를 찾아보세요.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_preprocessed['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec719685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 분할\n",
    "# - train/val_test 비율 8:1:1\n",
    "\n",
    "train_size = int(len(dataset_preprocessed) * 0.8)\n",
    "val_size = int(len(dataset_preprocessed) * 0.1)\n",
    "test_size = int(len(dataset_preprocessed) * 0.1)\n",
    "\n",
    "train_dataset = dataset_preprocessed.select(range(train_size))  # 앞 80% train\n",
    "val_dataset = dataset_preprocessed.select(range(train_size, train_size + val_size))  # 그 다음 10% val\n",
    "test_dataset = dataset_preprocessed.select(range(train_size + val_size, len(dataset_preprocessed))) # 나머지는 test\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35a205",
   "metadata": {},
   "source": [
    "## BaseModel 학습전 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1777220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt(질문)를 넣고, 모델이 답변을 생성하도록 하는 함수\n",
    "def generate_response(model, tokenizer, question):\n",
    "    prompt = question\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)  # prompt를 토큰화 후, 장치로 이동\n",
    "\n",
    "    with torch.no_grad():              # 추론 모드\n",
    "        outputs = model.generate(      # 텍스트 생성\n",
    "            **inputs,                  # input_ids / attention_mask\n",
    "            max_length = 1024,         # 생성을 포함한 최대 토큰 길이\n",
    "            do_sample = True,          # 샘플링 생성(매번 결과가 다르다)\n",
    "            top_k = 50,                # 확률 상위 k개 토큰 중에서 선택\n",
    "            top_p = 0.95,              # 누적 확률 p 범위 내 토큰 중에서 선택\n",
    "            temperature = 0.5,         # 창의성\n",
    "            num_return_sequences = 1,  # 답변 1개\n",
    "            eos_token_id = tokenizer.eos_token_id,  # 종료 토큰\n",
    "            pad_token_id = tokenizer.pad_token_id   # 패딩 토큰\n",
    "        )\n",
    "        generated_text = tokenizer.decode(outputs[0])      # 생성된 토큰을 문자열로 디코딩\n",
    "        return generated_text.replace(prompt, '').strip()  # prompt 제거 후 답변만 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "469e3ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[질문]:\n",
      "<|im_start|>system\n",
      "당신은 인공지능 어시스턴트입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "알리사와 아비게일은 과학 프로젝트를 위해 빈 캔 100개를 모아야 합니다. 오늘 현재 알리사는 빈 캔을 몇 개 모았고, 아비게일은 빈 캔 43개를 모았습니다. 그들은 빈 캔을 27개 더 모아야 합니다. 지금까지 알리사가 모은 빈 캔은 몇 개인가요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "[정답]:\n",
      "알리사가 얼마나 많은 빈 캔을 모았는지 알아내려면, 아비게일이 모은 캔의 수와 프로젝트에 필요한 총 캔 수에서 아직 모아야 할 캔의 수를 빼야 합니다.\n",
      "\n",
      "알리사의 캔 + 아비게일의 캔 + 아직 필요한 캔 = 필요한 총 캔 수입니다.\n",
      "알리사의 캔 = 필요한 총 캔 수 - (아비게일의 캔 + 아직 필요한 캔)\n",
      "\n",
      "우리는 이것을 알고 있습니다:\n",
      "필요한 총 캔 수 = 100\n",
      "아비게일의 캔 = 43\n",
      "아직 필요한 캔 = 27\n",
      "\n",
      "이제 숫자를 연결할 수 있습니다:\n",
      "\n",
      "알리사의 캔 = 100 - (43 + 27)\n",
      "알리사의 캔 = 100 - 70\n",
      "알리사의 캔 = 30\n",
      "\n",
      "알리사는 지금까지 빈 캔 30개를 모았습니다.\n",
      "\n",
      "[모델 생성 답변]:\n",
      "알리사가 지금까지 모은 빈 캔의 수를 알아내기 위해, 우리는 아비게일이 모은 빈 캔의 수에 27개를 더한 다음, 알리사와 아비게일이 모으려는 총 빈 캔의 수에서 빼야 합니다: 100 - (43 + 27) = 100 - 70 = 30 그러므로, 알리사가 지금까지 모은 빈 캔은 30개입니다.<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[질문]:\n",
      "<|im_start|>system\n",
      "당신은 인공지능 어시스턴트입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "알리사와 아비게일은 과학 프로젝트를 위해 빈 캔 100개를 모아야 합니다. 오늘 현재 알리사는 빈 캔 30개를 모았고 아비게일은 빈 캔을 몇 개 모았습니다. 두 사람은 빈 캔 27개를 더 모아야 합니다. 지금까지 아비게일은 몇 개의 빈 캔을 모았나요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "[정답]:\n",
      "알리사와 아비게일은 총 100개의 빈 캔을 모아야 합니다. 알리사는 이미 30개의 캔을 모았습니다. 100개의 목표에 도달하려면 27개의 캔을 더 모아야 합니다.\n",
      "\n",
      "아비게일이 얼마나 많은 캔을 모았는지 알아보려면, 알리사가 모은 캔의 수와 아직 모아야 할 캔의 수를 총 캔의 수에서 빼면 됩니다.\n",
      "\n",
      "따라서 아비게일이 모은 캔은 = 필요한 총 캔 수 - (알리사가 모은 캔 + 아직 모아야 할 캔)입니다.\n",
      "아비게일이 모은 캔 = 100 - (30 + 27)\n",
      "아비게일이 모은 캔 = 100 - 57\n",
      "아비게일이 모은 캔 = 43캔\n",
      "\n",
      "아비게일은 지금까지 빈 캔 43개를 모았습니다.\n",
      "\n",
      "[모델 생성 답변]:\n",
      "현재 알리사가 수집한 빈캔 수를 알 수 있어요: 30개. 두 사람이 수집해야 할 총 빈캔 수는 100개예요. 따라서 아비게일이 수집해야 하는 빈캔 수를 구하려면 다음 방정식을 사용할 수 있어요: 100 (총 빈캔 수) = 30 (알리사가 수집한 빈캔 수) + x (아비게일이 수집한 빈캔 수) 방정식의 양쪽에서 30을 빼서 x를 고립시켜 보아요: 100 - 30 = x 70 = x 이제 아비게일이 수집해야 하는 빈캔 수를 구했으므로, 그들이 수집한 빈캔 수를 더해서 그들이 여전히 필요한 빈캔 수를 계산할 수 있어요: 30 (알리사가 수집한 빈캔 수) + 70 (아비게일이 수집한 빈캔 수) = 100 (두 사람이 수집해야 하는 총 빈캔 수) 이제 두 사람이 아직 필요로 하는 빈캔 수를 구할 수 있어요: 100 (두 사람이 수집해야 하는 총 빈캔 수) - 70 (아비게일이 수집한 빈캔 수) = 30 (두 사람이 아직 필요로 하는 빈캔 수) 따라서 아비게일은 70개의 빈캔을 수집했어요.<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[질문]:\n",
      "<|im_start|>system\n",
      "당신은 인공지능 어시스턴트입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "이반은 주사위 몇 개를 가지고 있습니다. 제리는 이반보다 두 배 많은 주사위를 가지고 있습니다. 두 사람은 총 60개의 주사위를 가지고 있습니다. 이반은 몇 개의 주사위를 가지고 있을까요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "[정답]:\n",
      " 이반이 가지고 있는 주사위의 수를 \\( x \\)`로 표시해 봅시다. 문제에 따르면 제리는 이반보다 두 배 많은 주사위를 가지고 있으므로 제리는 \\( 2x \\) 주사위를 가지고 있습니다.\n",
      "\n",
      "두 사람이 함께 가지고 있는 주사위의 총 개수는 60개입니다. 따라서 방정식을 쓸 수 있습니다:\n",
      "\n",
      "\\[ x + 2x = 60 \\]\n",
      "\n",
      "유사한 항을 결합하면 다음과 같습니다:\n",
      "\n",
      "\\[ 3x = 60 \\]\n",
      "\n",
      "이제 방정식의 양쪽을 3으로 나누어 \\( x \\)`를 풀 수 있습니다:\n",
      "\n",
      "\\[ x = \\frac{60}{3} \\]\n",
      "\n",
      "\\[ x = 20 \\]\n",
      "\n",
      "따라서 이반은 20개의 주사위를 가지고 있습니다.\n",
      "\n",
      "[모델 생성 답변]:\n",
      "이 문제를 해결하기 위해, 먼저 변수를 사용하여 주어진 정보를 정리해 보겠어요. Let's say Ivan has x number of dice. Since Jerry has twice as many dice as Ivan, we can represent the number of dice Jerry has as 2x. Together, they have a total of 60 dice, so we can write the equation: x + 2x = 60. Now, we combine like terms: 3x = 60. To find the value of x, we divide both sides of the equation by 3: x = 60 / 3. Therefore, x = 20. So, Ivan has 20 dice.<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_subset = test_dataset.select(range(3))\n",
    "\n",
    "for i, example in enumerate(test_subset):  # 샘플 3개를 순회\n",
    "    question = example['prompt']\n",
    "    answer = example['chosen']    # 선호 답변(데이터셋에서는 정답)\n",
    "\n",
    "    print(f'[질문]:\\n{question}')\n",
    "    print(f'\\n[정답]:\\n{answer}')\n",
    "\n",
    "    generated_answer = generate_response(model, tokenizer, question)\n",
    "    print(f'\\n[모델 생성 답변]:\\n{generated_answer}')\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41c76b",
   "metadata": {},
   "source": [
    "## DPO 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78663358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,523,136 || all params: 7,618,139,648 || trainable%: 0.0331\n"
     ]
    }
   ],
   "source": [
    "# LoRA(PEFT) 적용으로 정책모델 학습 파라미터 축소 확인\n",
    "from peft import LoraConfig, get_peft_model  # LoRA 설정 / PEFT 적용 함수\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,                   # LoRA 랭크 (추가로 학습할 저차원 행렬 크기)\n",
    "    lora_alpha = 16,         # LoRA 스케일 (업데이트 강도 조절)\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],  # LoRA에 붙일 모듈 (어텐션의 Q/V 프로젝션)\n",
    "    lora_dropout = 0.05,     # 과적합 완화\n",
    "    bias = 'none',           # bias 학습 제외\n",
    "    task_type = 'CAUSAL_LM'  # 작업 유형 (디코더 LM)\n",
    ")\n",
    "\n",
    "# 정책 모델 (학습대상)\n",
    "model = get_peft_model(model, lora_config)  # 기존 모델에 LoRA 적용 (PEFT 모델)\n",
    "\n",
    "model.print_trainable_parameters()  # 학습가능한 파라미터 수/비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f89f9423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850342a8a4fb462ab10caf2d4df5519b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 참조모델 : 정책모델이 얼마나 바뀌었는지 비교 기준이 되는 고정 모델\n",
    "# - chosen/rejecdted 답변에 대한 생성확률 비교값 제공\n",
    "\n",
    "# 참조모델 (학습하지 않은 기본모델)\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype = torch.bfloat16,    # 메모리 절약 및 연산 효율성\n",
    "    device_map = \"auto\",       # CPU/GPU 자동 배치\n",
    "    trust_remote_code = True   # 모델 저장소에 있는 커스텀 모델 코드 허용\n",
    ")\n",
    "\n",
    "# 학습방지 처리\n",
    "reference_model.eval()  # 평가모드 전환\n",
    "for param in reference_model.parameters():  # 모든 파라미터 순회\n",
    "    param.requires_grad = False             # gradient 계산 / 업데이트 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d0bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_418/381938084.py:7: FutureWarning: `max_prompt_length` is deprecated and will be removed in version 0.29.0. We recommend filtering out overlong prompts from your dataset before passing it to the trainer instead of using this parameter.\n",
      "  training_args = DPOConfig(\n"
     ]
    }
   ],
   "source": [
    "# DPOConfig 설정\n",
    "from trl import DPOTrainer, DPOConfig  # DPO 학습 클래스 / 설정 클래스\n",
    "\n",
    "hub_model_id = 'capybaraOh/vclm-korean-7b-orca-math-korean-dpo'  # 학습 완료 모델을 올릴 Hub 저장소\n",
    "\n",
    "# DPO 학습 하이퍼파라미터/로깅/저장 설정\n",
    "training_args = DPOConfig(\n",
    "    output_dir='vclm-korean-7b-orca-math-korean-dpo',  # 체크포인트/로그 저장 폴더\n",
    "    num_train_epochs=1,             # 전체 데이터를 1번 반복 학습\n",
    "    per_device_train_batch_size=2,  # GPU 1개당 배치 크기\n",
    "    gradient_accumulation_steps=4,  # 4번 누적 후 업데이트(실제 배치 효과: 2*4=8)\n",
    "    learning_rate=5e-5,             # 학습률\n",
    "    eval_strategy=\"steps\",          # 일정 step마다 평가 수행\n",
    "    save_strategy=\"steps\",          # 일정 step마다 저장 수행\n",
    "    logging_steps=50,               # 50 step마다 학습 로그 출력\n",
    "    fp16=False,                     # fp16 비활성화(여기서는 bf16 사용)\n",
    "    bf16=True,                      # bfloat16 사용(지원 GPU에서 안정적/빠름)\n",
    "    tf32=True,                      # Ampere 이상에서 matmul 가속 옵션(정밀도 약간 완화)\n",
    "    beta=0.1,                       # DPO의 beta(선호 강도 조절)\n",
    "    max_length=512,                 # prompt+답변을 포함한 최대 길이\n",
    "    max_prompt_length=256,          # prompt에 할당할 최대 길이\n",
    "    remove_unused_columns=False,    # DPO에 필요한 컬럼이 제거되지 않도록 유지\n",
    "    push_to_hub=True,               # 학습 결과를 Hugging Face Hub로 업로드\n",
    "    hub_model_id=hub_model_id,      # 업로드할 저장소 이름\n",
    "    hub_strategy=\"end\",             # 학습 끝난 뒤 한 번만 업로드\n",
    "    report_to=['wandb']             # wandb로 학습 로그 전송\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "971bdb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf328be15924a8188718aeb61733bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe25671606d94ea8ab72b3467057f736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82ea4c63dd74e05ba119e28204eb634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5b045d3ff74fc699a2e5687d8e2e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c6a1c6510540289cbdc8cd89084930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c513903d174de0862751c99aba2151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Create a new API key at: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Store your API key securely and do not share it.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste your API key and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapybara-ohgiraffers\u001b[0m (\u001b[33mcapybara-ohgiraffers-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20260213_050821-1x3xb08p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capybara-ohgiraffers-/huggingface/runs/1x3xb08p' target=\"_blank\">laced-puddle-2</a></strong> to <a href='https://wandb.ai/capybara-ohgiraffers-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capybara-ohgiraffers-/huggingface' target=\"_blank\">https://wandb.ai/capybara-ohgiraffers-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capybara-ohgiraffers-/huggingface/runs/1x3xb08p' target=\"_blank\">https://wandb.ai/capybara-ohgiraffers-/huggingface/runs/1x3xb08p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 1:05:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.579535</td>\n",
       "      <td>0.392952</td>\n",
       "      <td>0.702219</td>\n",
       "      <td>-0.206515</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.908734</td>\n",
       "      <td>-179.547287</td>\n",
       "      <td>-234.314026</td>\n",
       "      <td>1.139387</td>\n",
       "      <td>1.017785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.249898</td>\n",
       "      <td>0.154205</td>\n",
       "      <td>-0.024517</td>\n",
       "      <td>-3.058099</td>\n",
       "      <td>0.963000</td>\n",
       "      <td>3.033582</td>\n",
       "      <td>-186.814636</td>\n",
       "      <td>-262.829865</td>\n",
       "      <td>1.191236</td>\n",
       "      <td>1.057575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.110530</td>\n",
       "      <td>0.100609</td>\n",
       "      <td>-0.676667</td>\n",
       "      <td>-6.047880</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>5.371213</td>\n",
       "      <td>-193.336121</td>\n",
       "      <td>-292.727692</td>\n",
       "      <td>1.208654</td>\n",
       "      <td>1.066885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.068252</td>\n",
       "      <td>0.095909</td>\n",
       "      <td>-1.238648</td>\n",
       "      <td>-7.874978</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>6.636330</td>\n",
       "      <td>-198.955948</td>\n",
       "      <td>-310.998657</td>\n",
       "      <td>1.213631</td>\n",
       "      <td>1.065149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.091005</td>\n",
       "      <td>0.093133</td>\n",
       "      <td>-1.170401</td>\n",
       "      <td>-8.480999</td>\n",
       "      <td>0.966000</td>\n",
       "      <td>7.310599</td>\n",
       "      <td>-198.273453</td>\n",
       "      <td>-317.058868</td>\n",
       "      <td>1.196639</td>\n",
       "      <td>1.047680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.057171</td>\n",
       "      <td>0.093310</td>\n",
       "      <td>-1.267962</td>\n",
       "      <td>-9.321945</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>8.053984</td>\n",
       "      <td>-199.249100</td>\n",
       "      <td>-325.468323</td>\n",
       "      <td>1.182520</td>\n",
       "      <td>1.031703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.045625</td>\n",
       "      <td>0.110507</td>\n",
       "      <td>-1.798110</td>\n",
       "      <td>-11.051714</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>9.253606</td>\n",
       "      <td>-204.550568</td>\n",
       "      <td>-342.765991</td>\n",
       "      <td>1.175144</td>\n",
       "      <td>1.019505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.046910</td>\n",
       "      <td>0.112846</td>\n",
       "      <td>-2.375344</td>\n",
       "      <td>-12.288463</td>\n",
       "      <td>0.971000</td>\n",
       "      <td>9.913117</td>\n",
       "      <td>-210.322891</td>\n",
       "      <td>-355.133545</td>\n",
       "      <td>1.167809</td>\n",
       "      <td>1.005387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.092997</td>\n",
       "      <td>-2.104456</td>\n",
       "      <td>-11.990234</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>9.885779</td>\n",
       "      <td>-207.614029</td>\n",
       "      <td>-352.151184</td>\n",
       "      <td>1.162987</td>\n",
       "      <td>1.000989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.091108</td>\n",
       "      <td>0.077229</td>\n",
       "      <td>-1.583007</td>\n",
       "      <td>-11.376247</td>\n",
       "      <td>0.973000</td>\n",
       "      <td>9.793241</td>\n",
       "      <td>-202.399536</td>\n",
       "      <td>-346.011322</td>\n",
       "      <td>1.166556</td>\n",
       "      <td>1.005637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.085276</td>\n",
       "      <td>0.081225</td>\n",
       "      <td>-1.723009</td>\n",
       "      <td>-12.003674</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>10.280666</td>\n",
       "      <td>-203.799530</td>\n",
       "      <td>-352.285583</td>\n",
       "      <td>1.159181</td>\n",
       "      <td>0.996645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.060365</td>\n",
       "      <td>0.082287</td>\n",
       "      <td>-1.705695</td>\n",
       "      <td>-12.259316</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>10.553622</td>\n",
       "      <td>-203.626404</td>\n",
       "      <td>-354.842041</td>\n",
       "      <td>1.148952</td>\n",
       "      <td>0.986717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.044739</td>\n",
       "      <td>0.075813</td>\n",
       "      <td>-2.049741</td>\n",
       "      <td>-13.234404</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>11.184662</td>\n",
       "      <td>-207.066879</td>\n",
       "      <td>-364.592926</td>\n",
       "      <td>1.139739</td>\n",
       "      <td>0.971952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0.079363</td>\n",
       "      <td>-2.112886</td>\n",
       "      <td>-13.548509</td>\n",
       "      <td>0.973000</td>\n",
       "      <td>11.435621</td>\n",
       "      <td>-207.698303</td>\n",
       "      <td>-367.733948</td>\n",
       "      <td>1.134220</td>\n",
       "      <td>0.962102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.055678</td>\n",
       "      <td>0.093882</td>\n",
       "      <td>-2.343226</td>\n",
       "      <td>-14.186480</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>11.843254</td>\n",
       "      <td>-210.001724</td>\n",
       "      <td>-374.113647</td>\n",
       "      <td>1.126798</td>\n",
       "      <td>0.956505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.064902</td>\n",
       "      <td>0.083090</td>\n",
       "      <td>-2.175181</td>\n",
       "      <td>-13.970621</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>11.795441</td>\n",
       "      <td>-208.321274</td>\n",
       "      <td>-371.955048</td>\n",
       "      <td>1.128404</td>\n",
       "      <td>0.956900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.064242</td>\n",
       "      <td>0.080663</td>\n",
       "      <td>-2.267204</td>\n",
       "      <td>-14.221276</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>11.954071</td>\n",
       "      <td>-209.241501</td>\n",
       "      <td>-374.461639</td>\n",
       "      <td>1.123815</td>\n",
       "      <td>0.951335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.030117</td>\n",
       "      <td>0.088595</td>\n",
       "      <td>-2.364085</td>\n",
       "      <td>-14.473271</td>\n",
       "      <td>0.973000</td>\n",
       "      <td>12.109186</td>\n",
       "      <td>-210.210312</td>\n",
       "      <td>-376.981598</td>\n",
       "      <td>1.124672</td>\n",
       "      <td>0.950911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.117363</td>\n",
       "      <td>0.086640</td>\n",
       "      <td>-2.361023</td>\n",
       "      <td>-14.498412</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>12.137388</td>\n",
       "      <td>-210.179688</td>\n",
       "      <td>-377.233002</td>\n",
       "      <td>1.123868</td>\n",
       "      <td>0.950048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.057685</td>\n",
       "      <td>0.086993</td>\n",
       "      <td>-2.338725</td>\n",
       "      <td>-14.457221</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>12.118495</td>\n",
       "      <td>-209.956680</td>\n",
       "      <td>-376.821075</td>\n",
       "      <td>1.123392</td>\n",
       "      <td>0.949974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.10134319376945496, metrics={'train_runtime': 3939.7714, 'train_samples_per_second': 2.031, 'train_steps_per_second': 0.254, 'total_flos': 0.0, 'train_loss': 0.10134319376945496, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DPO Trainer 생성 (정책모델 vs 참조모델 비교 학습)\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,                  # 정책모델\n",
    "    ref_model = reference_model,    # 참조모델\n",
    "    args = training_args,           # 학습 설정\n",
    "    train_dataset = train_dataset,  # 학습 데이터 (prompt/chosen/rejected)\n",
    "    eval_dataset = val_dataset,     # 검증 데이터\n",
    "    processing_class = tokenizer    # 토큰화 처리\n",
    ")\n",
    "\n",
    "dpo_trainer.train()  # DPO학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5b3bd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3742755d641348f4b5d12ebf2036592e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417d148b51df42dfbad127858eb3b26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/capybaraOh/vclm-korean-7b-orca-math-korean-dpo/commit/b2eed7ab89b73fc43812cfec4e2058173439afcd', commit_message='Commit!', commit_description='', oid='b2eed7ab89b73fc43812cfec4e2058173439afcd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/capybaraOh/vclm-korean-7b-orca-math-korean-dpo', endpoint='https://huggingface.co', repo_type='model', repo_id='capybaraOh/vclm-korean-7b-orca-math-korean-dpo'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습이 중간에 끊어졌거나 혹은 자동으로 업로드되지 않은 경우\n",
    "dpo_trainer.push_to_hub('Commit!')  # 현재 학습된 체크포인트/모델을 Hub에 커밋 메시지와 함께 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3985271b",
   "metadata": {},
   "source": [
    "## DPO Model 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36773271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[질문]:\n",
      "<|im_start|>system\n",
      "당신은 인공지능 어시스턴트입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "알리사와 아비게일은 과학 프로젝트를 위해 빈 캔 100개를 모아야 합니다. 오늘 현재 알리사는 빈 캔을 몇 개 모았고, 아비게일은 빈 캔 43개를 모았습니다. 그들은 빈 캔을 27개 더 모아야 합니다. 지금까지 알리사가 모은 빈 캔은 몇 개인가요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "[정답]:\n",
      "알리사가 얼마나 많은 빈 캔을 모았는지 알아내려면, 아비게일이 모은 캔의 수와 프로젝트에 필요한 총 캔 수에서 아직 모아야 할 캔의 수를 빼야 합니다.\n",
      "\n",
      "알리사의 캔 + 아비게일의 캔 + 아직 필요한 캔 = 필요한 총 캔 수입니다.\n",
      "알리사의 캔 = 필요한 총 캔 수 - (아비게일의 캔 + 아직 필요한 캔)\n",
      "\n",
      "우리는 이것을 알고 있습니다:\n",
      "필요한 총 캔 수 = 100\n",
      "아비게일의 캔 = 43\n",
      "아직 필요한 캔 = 27\n",
      "\n",
      "이제 숫자를 연결할 수 있습니다:\n",
      "\n",
      "알리사의 캔 = 100 - (43 + 27)\n",
      "알리사의 캔 = 100 - 70\n",
      "알리사의 캔 = 30\n",
      "\n",
      "알리사는 지금까지 빈 캔 30개를 모았습니다.\n",
      "\n",
      "[모델 생성 답변]:\n",
      "이 문제를 해결하기 위해 두 가지 정보를 알아야 합니다: 알리사가 지금까지 모은 빈캔의 수와 아비게일이 모은 총 빈캔 수입니다.\n",
      "\n",
      "주어진 정보에 따르면:\n",
      "- 아비게일은 43개의 빈캔을 모았습니다.\n",
      "- 알리사와 아비게일은 함께 27개의 빈캔을 더 모아야 합니다.\n",
      "- 총 빈캔 수는 100개입니다.\n",
      "\n",
      "먼저 아비게일이 더 이상 모으지 않는 가정 하에 알리사와 아비게일이 이미 모은 총 빈캔 수를 계산합니다:\n",
      "\n",
      "빈캔 총 개수 = 알리사가 모은 빈캔 + 아비게일이 모은 빈캔\n",
      "100 - 27(더해야 할 빈캔) = 알리사가 모은 빈캔 + 43\n",
      "\n",
      "이제 아비게일이 이미 모은 것과 더해야 할 빈캔을 제외한 총 빈캔 수를 알 수 있습니다:\n",
      "\n",
      "73 = 알리사가 모은 빈캔 + 43\n",
      "\n",
      "이제 알리사가 모은 빈캔의 수를 구합니다:\n",
      "\n",
      "알리사가 모은 빈캔 = 73 - 43\n",
      "알리사가 모은 빈캔 = 30\n",
      "\n",
      "따라서 알리사는 지금까지 30개의 빈캔을 모았습니다.<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[질문]:\n",
      "<|im_start|>system\n",
      "당신은 인공지능 어시스턴트입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "알리사와 아비게일은 과학 프로젝트를 위해 빈 캔 100개를 모아야 합니다. 오늘 현재 알리사는 빈 캔 30개를 모았고 아비게일은 빈 캔을 몇 개 모았습니다. 두 사람은 빈 캔 27개를 더 모아야 합니다. 지금까지 아비게일은 몇 개의 빈 캔을 모았나요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "[정답]:\n",
      "알리사와 아비게일은 총 100개의 빈 캔을 모아야 합니다. 알리사는 이미 30개의 캔을 모았습니다. 100개의 목표에 도달하려면 27개의 캔을 더 모아야 합니다.\n",
      "\n",
      "아비게일이 얼마나 많은 캔을 모았는지 알아보려면, 알리사가 모은 캔의 수와 아직 모아야 할 캔의 수를 총 캔의 수에서 빼면 됩니다.\n",
      "\n",
      "따라서 아비게일이 모은 캔은 = 필요한 총 캔 수 - (알리사가 모은 캔 + 아직 모아야 할 캔)입니다.\n",
      "아비게일이 모은 캔 = 100 - (30 + 27)\n",
      "아비게일이 모은 캔 = 100 - 57\n",
      "아비게일이 모은 캔 = 43캔\n",
      "\n",
      "아비게일은 지금까지 빈 캔 43개를 모았습니다.\n",
      "\n",
      "[모델 생성 답변]:\n",
      "현재 알리사가 총 빈캔 100개 목표 중에 30개를 수집했으므로, 아비게일과 함께 두 사람은 지금까지 100 - 30 = 70개의 빈캔을 수집했습니다.\n",
      "\n",
      "두 사람이 아직 모아야 하는 빈캔은 27개입니다. 따라서 아비게일과 알리사가 이미 수집한 70개를 모두 모은 빈캔의 총 개수는 70 + 27 = 97개입니다.\n",
      "\n",
      "이제 알리사가 이미 수집한 빈캔 30개를 빼면 아비게일이 수집한 빈캔 수를 알 수 있습니다: 97 - 30 = 67개입니다.\n",
      "\n",
      "따라서 아비게일은 지금까지 67개의 빈캔을 수집했습니다.<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[질문]:\n",
      "<|im_start|>system\n",
      "당신은 인공지능 어시스턴트입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "이반은 주사위 몇 개를 가지고 있습니다. 제리는 이반보다 두 배 많은 주사위를 가지고 있습니다. 두 사람은 총 60개의 주사위를 가지고 있습니다. 이반은 몇 개의 주사위를 가지고 있을까요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "[정답]:\n",
      " 이반이 가지고 있는 주사위의 수를 \\( x \\)`로 표시해 봅시다. 문제에 따르면 제리는 이반보다 두 배 많은 주사위를 가지고 있으므로 제리는 \\( 2x \\) 주사위를 가지고 있습니다.\n",
      "\n",
      "두 사람이 함께 가지고 있는 주사위의 총 개수는 60개입니다. 따라서 방정식을 쓸 수 있습니다:\n",
      "\n",
      "\\[ x + 2x = 60 \\]\n",
      "\n",
      "유사한 항을 결합하면 다음과 같습니다:\n",
      "\n",
      "\\[ 3x = 60 \\]\n",
      "\n",
      "이제 방정식의 양쪽을 3으로 나누어 \\( x \\)`를 풀 수 있습니다:\n",
      "\n",
      "\\[ x = \\frac{60}{3} \\]\n",
      "\n",
      "\\[ x = 20 \\]\n",
      "\n",
      "따라서 이반은 20개의 주사위를 가지고 있습니다.\n",
      "\n",
      "[모델 생성 답변]:\n",
      "이 문제를 해결하기 위해 변수를 사용하여 방정식을 설정해 보겠습니다.\n",
      "\n",
      "에반의 주사위 수를 x라고 부르겠습니다. 제리가 이반보다 주사위가 두 배 많으므로 제리는 2x개의 주사위를 가지고 있습니다.\n",
      "\n",
      "두 사람이 함께 가진 주사위 총 개수는 다음과 같습니다:\n",
      "x (Evan의 주사위) + 2x (Jerry의 주사위) = 60\n",
      "\n",
      "이것은 다음 방정식으로 단순화됩니다:\n",
      "3x = 60\n",
      "\n",
      "양변을 3으로 나누면 x의 값을 구할 수 있습니다:\n",
      "x = 60 / 3\n",
      "x = 20\n",
      "\n",
      "따라서 이반은 20개의 주사위를 가지고 있습니다.<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_subset = test_dataset.select(range(3))\n",
    "\n",
    "for i, example in enumerate(test_subset):  # 샘플 3개를 순회\n",
    "    question = example['prompt']\n",
    "    answer = example['chosen']    # 선호 답변(데이터셋에서는 정답)\n",
    "\n",
    "    print(f'[질문]:\\n{question}')\n",
    "    print(f'\\n[정답]:\\n{answer}')\n",
    "\n",
    "    generated_answer = generate_response(model, tokenizer, question)\n",
    "    print(f'\\n[모델 생성 답변]:\\n{generated_answer}')\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17594e55",
   "metadata": {},
   "source": [
    "## DPO 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5f26bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# prompt에 대해 response가 얼마나 그럴듯한지(평균 log_prob) 계산하는 함수\n",
    "def calculate_log_prob(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    주어진 prompt에 대한 response의 log probability(평균)를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        model: HuggingFace AutoModelForCausalLM (or similar)\n",
    "        tokenizer: HuggingFace AutoTokenizer\n",
    "        prompt (str): 프롬프트 텍스트 \n",
    "        response (str): 응답 텍스트\n",
    "\n",
    "    Returns:\n",
    "        float: response 토큰들의 평균 log probability\n",
    "    \"\"\"\n",
    "    full_text = prompt + \" \" + response  # prompt + 답변\n",
    "    inputs = tokenizer(\n",
    "        full_text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,      # 최대 글자 길이 이상시 자름\n",
    "        max_length=512        # 최대 토큰 길이\n",
    "    ).to(model.device)  # 토큰화시키고 GPU/CPU로 이동\n",
    "    \n",
    "    input_ids = inputs['input_ids']  # 토큰 id들\n",
    "    attention_mask = inputs['attention_mask']  # 패딩 무시용 마스크\n",
    "\n",
    "    # prompt만 따로 토큰화\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    prompt_len = prompt_tokens[\"input_ids\"].shape[1]  # prompt 토큰 길이 (어디서부터 response인지 경계)\n",
    "\n",
    "    # 추론 모드(그래프 미생성 / 계산만 진행) : 메모리 절약\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # 순전파\n",
    "        logits = outputs.logits  # 각 위치별로 다음 토큰 후보 점수표\n",
    "    \n",
    "    shift_logits = logits[..., :-1, :].contiguous()  # 마지막 위치 제외(다음 토큰 예측용 길이 맞추기 위해서)\n",
    "    shift_labels = input_ids[..., 1:].contiguous()   # 정답 토큰은 한칸 왼쪽으로 당김 (=다음 토큰)\n",
    "\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)  # 점수표 -> log 확률포로 변환\n",
    "\n",
    "    true_log_probs = torch.gather(log_probs, 2, shift_labels.unsqueeze(-1)).squeeze(-1)  # (batch, seq_len-1, 1) -> (batch, seq_len-1)\n",
    "\n",
    "    seq_len = shift_labels.shape[1]  # 실제 점수 길이 (seq_len-1)\n",
    "    if prompt_len >= seq_len:        # prompt가 너무 길어서 response 구간이 거의 없을 때\n",
    "        valid_log_probs = true_log_probs[:, -1:]  # 마지막 차원값만 사용 (예외 처리)\n",
    "    else:\n",
    "        start_idx = max(0, prompt_len -1)  # response 첫 토큰 점수 위치(= prompt의 마지막 토큰 자리)\n",
    "        valid_log_probs = true_log_probs[:, start_idx:]  # prompt 점수 제외하고 response 점수만 사용\n",
    "    \n",
    "    avg_log_probs = valid_log_probs.mean().item()  # response 점수 평균\n",
    "\n",
    "    return avg_log_probs  # 평균 log_prob 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a815dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300개 샘플에 대해 정확도 계산 시작\n",
      " - 진행률 : 10/300 (현재 정확도: 70.00%)\n",
      " - 진행률 : 20/300 (현재 정확도: 70.00%)\n",
      " - 진행률 : 30/300 (현재 정확도: 80.00%)\n",
      " - 진행률 : 40/300 (현재 정확도: 75.00%)\n",
      " - 진행률 : 50/300 (현재 정확도: 76.00%)\n",
      " - 진행률 : 60/300 (현재 정확도: 71.67%)\n",
      " - 진행률 : 70/300 (현재 정확도: 70.00%)\n",
      " - 진행률 : 80/300 (현재 정확도: 71.25%)\n",
      " - 진행률 : 90/300 (현재 정확도: 73.33%)\n",
      " - 진행률 : 100/300 (현재 정확도: 73.00%)\n",
      " - 진행률 : 110/300 (현재 정확도: 72.73%)\n",
      " - 진행률 : 120/300 (현재 정확도: 74.17%)\n",
      " - 진행률 : 130/300 (현재 정확도: 74.62%)\n",
      " - 진행률 : 140/300 (현재 정확도: 75.71%)\n",
      " - 진행률 : 150/300 (현재 정확도: 75.33%)\n",
      " - 진행률 : 160/300 (현재 정확도: 76.25%)\n",
      " - 진행률 : 170/300 (현재 정확도: 77.06%)\n",
      " - 진행률 : 180/300 (현재 정확도: 76.11%)\n",
      " - 진행률 : 190/300 (현재 정확도: 76.32%)\n",
      " - 진행률 : 200/300 (현재 정확도: 77.50%)\n",
      " - 진행률 : 210/300 (현재 정확도: 77.62%)\n",
      " - 진행률 : 220/300 (현재 정확도: 77.73%)\n",
      " - 진행률 : 230/300 (현재 정확도: 77.39%)\n",
      " - 진행률 : 240/300 (현재 정확도: 78.33%)\n",
      " - 진행률 : 250/300 (현재 정확도: 77.60%)\n",
      " - 진행률 : 260/300 (현재 정확도: 78.46%)\n",
      " - 진행률 : 270/300 (현재 정확도: 78.89%)\n",
      " - 진행률 : 280/300 (현재 정확도: 77.86%)\n",
      " - 진행률 : 290/300 (현재 정확도: 77.93%)\n",
      " - 진행률 : 300/300 (현재 정확도: 77.67%)\n",
      "0.7766666666666666\n"
     ]
    }
   ],
   "source": [
    "def calcualte_preference_accuracy(model, tokenizer, dataset, num_samples=100):\n",
    "    \"\"\"\n",
    "    모델이 선호(chosen) 답변에 비선호(rejected) 답변보다 더 높은 확률을 부여하는지를 평가\n",
    "    \"\"\"\n",
    "    correct = 0  # chosen을 더 높게 준 횟수\n",
    "    total = min(num_samples, len(dataset))  # 평가 샘플 수\n",
    "\n",
    "    print(f\"{total}개 샘플에 대해 정확도 계산 시작\")\n",
    "\n",
    "    model.eval()  # 평가모드\n",
    "\n",
    "    for idx in range(total):\n",
    "        example = dataset[idx]  # 한개의 샘플씩 가져옴\n",
    "\n",
    "        prompt = example['prompt']      # 프롬프트\n",
    "        chosen = example['chosen']      # 선호답변\n",
    "        rejected = example['rejected']  # 비선호답변\n",
    "\n",
    "        chosen_score = calculate_log_prob(model, tokenizer, prompt, chosen)      # 선호답변 평균 log_prob\n",
    "        rejected_score = calculate_log_prob(model, tokenizer, prompt, rejected)  # 비선호답변 평균 log_prob\n",
    "\n",
    "        if chosen_score > rejected_score:  # 선호답변 평균이 비선호답변 평균보다 높으면\n",
    "            correct += 1\n",
    "\n",
    "        if (idx + 1) % 10 == 0:  # 10개 샘플마다\n",
    "            print(f' - 진행률 : {idx+1}/{total} (현재 정확도: {correct/(idx+1) * 100:.2f}%)')\n",
    "    \n",
    "    accuracy = correct / total  # 최종 정확도\n",
    "    return accuracy\n",
    "\n",
    "test_accuracy = calcualte_preference_accuracy(model, tokenizer, test_dataset, num_samples=300)  # 테스트 셋으로 300개 평간\n",
    "print(test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
